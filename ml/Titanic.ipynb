{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "32359e95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 0.0 start & check\n",
    "import pandas\n",
    "titanic0 = pandas.read_csv(\"train.csv\")\n",
    "# titanic0.head()\n",
    "# print(titanic0.describe())\n",
    "# The 'Age' column contains missing values.\n",
    "# Three methods to fill in :\n",
    "# 1. Using the median age for all passengers \n",
    "# 2. Using a regression model to predict age based on other features \n",
    "# 3. Using random sampling from existing age values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c4c5f9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.1 1st method, average\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "titanic = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Calculate the mean age\n",
    "mean_age = titanic['Age'].mean()\n",
    "\n",
    "# Fill missing Age values with the average (mean) of the existing Age values using loc\n",
    "titanic.loc[titanic['Age'].isnull(), 'Age'] = mean_age\n",
    "\n",
    "# Save the DataFrame with filled missing values to a new file named train0.csv\n",
    "titanic.to_csv(\"train0.csv\", index=False)\n",
    "\n",
    "# Check if all missing values have been filled\n",
    "# print(titanic['Age'].isnull().sum())  # If output is 0, all missing values are filled\n",
    "# print(titanic.head())  # View the first few rows to confirm the data filling\n",
    "\n",
    "# Check the new CSV file\n",
    "titanic_check = pd.read_csv(\"train0.csv\")\n",
    "#print(titanic_check.head())\n",
    "#print(titanic_check.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "62c0e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.2 2nd method,regression\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the data\n",
    "titanic = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Create a boolean mask to identify missing values in the 'Age' column\n",
    "age_missing = titanic['Age'].isnull()\n",
    "\n",
    "# Select features to predict age (excluding 'Age' itself)\n",
    "features = titanic[['Pclass', 'SibSp', 'Parch', 'Fare']]\n",
    "\n",
    "# Split data into training set (with age) and prediction set (without age)\n",
    "train_data = features[~age_missing]   # Data used for training (rows without missing values)\n",
    "train_target = titanic['Age'][~age_missing]  # Age labels for training data\n",
    "predict_data = features[age_missing]  # Data used for predicting missing values\n",
    "\n",
    "# Train a linear regression model using the rows without missing age values\n",
    "model = LinearRegression()\n",
    "model.fit(train_data, train_target)\n",
    "\n",
    "# Predict the missing age values\n",
    "predicted_ages = model.predict(predict_data)\n",
    "\n",
    "# Create a new copy of the original DataFrame and fill in the missing age values\n",
    "titanic_filled = titanic.copy()\n",
    "titanic_filled.loc[age_missing, 'Age'] = predicted_ages\n",
    "\n",
    "# Save the DataFrame with filled missing values to a new file named train1.csv\n",
    "titanic_filled.to_csv(\"train1.csv\", index=False)\n",
    "\n",
    "# Check if all missing values have been filled in the new DataFrame\n",
    "# print(titanic_filled['Age'].isnull().sum())  # If output is 0, all missing values are filled\n",
    "# print(titanic_filled.head())  # View the first few rows to confirm the data filling\n",
    "\n",
    "# check2\n",
    "import pandas\n",
    "titanic = pandas.read_csv(\"train1.csv\")\n",
    "# titanic.head()\n",
    "# print(titanic.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "43b75277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.3 3rd method:ramdom samples\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "titanic = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Create a boolean mask for missing Age values\n",
    "age_missing = titanic['Age'].isnull()\n",
    "\n",
    "# Use random sampling to fill in missing Age values\n",
    "# Fill missing values by randomly choosing from existing Age values\n",
    "titanic.loc[age_missing, 'Age'] = np.random.choice(titanic['Age'].dropna(), size=age_missing.sum())\n",
    "\n",
    "# Save the DataFrame with filled missing values to a new file named train2.csv\n",
    "titanic.to_csv(\"train2.csv\", index=False)\n",
    "\n",
    "# Check if all missing values have been filled\n",
    "# print(titanic['Age'].isnull().sum())  # If output is 0, all missing values are filled\n",
    "# print(titanic.head())  # View the first few rows to confirm the data filling\n",
    "\n",
    "#check 3\n",
    "import pandas\n",
    "titanic = pandas.read_csv(\"train2.csv\")\n",
    "# titanic.head()\n",
    "# print(titanic.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19b66678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(titanic[\"Sex\"].unique())\n",
    "#Replace all the occurences of male with the number O.\n",
    "\n",
    "titanic.loc[titanic[\"Sex\"]==\"male\",\"Sex\"]= 0\n",
    "titanic.loc[titanic[\"Sex\"]==\"female\",\"Sex\"]= 1\n",
    "print(titanic[\"Sex\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ae5eb807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "print(titanic[\"Embarked\"].unique())\n",
    "titanic.loc[titanic[\"Embarked\"]== \"S\",\"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"]== \"C\",\"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"]== \"Q\",\"Embarked\"] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6d516cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#use mode for filling the blanks\n",
    "\n",
    "# pd.set_option('future.no_silent_downcasting', True)\n",
    "\n",
    "most_frequent = titanic['Embarked'].mode()[0]\n",
    "print(most_frequent)\n",
    "titanic['Embarked'] = titanic['Embarked'].fillna(most_frequent)\n",
    "print(titanic['Embarked'].isnull().sum()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "04dc36f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.5387205387205387\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the predictor variables and the target variable\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize the logistic regression model with a maximum of 200 iterations\n",
    "alg = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Set up 3-fold cross-validation\n",
    "kf = KFold(n_splits=3, random_state=1, shuffle=True)\n",
    "\n",
    "# Initialize an empty list to store predictions from each fold\n",
    "all_predictions = []\n",
    "\n",
    "# Cross-validation loop\n",
    "for train_index, test_index in kf.split(titanic):\n",
    "    # Split the data into training predictors and target variable\n",
    "    train_predictors = titanic[predictors].iloc[train_index]\n",
    "    train_target = titanic[\"Survived\"].iloc[train_index]\n",
    "\n",
    "    # Standardize the training predictors\n",
    "    scaler = StandardScaler()\n",
    "    train_predictors = scaler.fit_transform(train_predictors)\n",
    "\n",
    "    # Fit the logistic regression model on the training data\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    \n",
    "    # Prepare the test predictors\n",
    "    test_predictors = titanic[predictors].iloc[test_index]\n",
    "    test_predictors = scaler.transform(test_predictors)\n",
    "    \n",
    "    # Get predicted probabilities for the test set (for class 1)\n",
    "    fold_predictions_proba = alg.predict_proba(test_predictors)[:, 1]  # Take the probability of class 1\n",
    "    \n",
    "    # Make final predictions based on a threshold (0.5)\n",
    "    fold_predictions = (fold_predictions_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Append the predictions from this fold to the list\n",
    "    all_predictions.append(fold_predictions)\n",
    "\n",
    "# Concatenate predictions from all folds into a single array\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "\n",
    "# Calculate the overall accuracy of the predictions\n",
    "accuracy = sum(all_predictions == titanic[\"Survived\"]) / len(all_predictions)\n",
    "\n",
    "# Print the overall accuracy\n",
    "print(\"Overall accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "627923ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated accuracy: 0.8036281463812692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "\n",
    "# randomForest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(rf_model, titanic[predictors], titanic[\"Survived\"], cv=5)\n",
    "print(\"Cross-validated accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a8353f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated accuracy: 0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define predictors and target variable\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize the Random Forest model with additional parameters\n",
    "rf_model = RandomForestClassifier(n_estimators=100,        # 125，manu test for the optimum result\n",
    "                                   min_samples_split=10,   # Minimum samples required to split an internal node\n",
    "                                   min_samples_leaf=5,     # Minimum samples required to be at a leaf node\n",
    "                                   random_state=1)\n",
    "\n",
    "# Set up KFold cross-validation\n",
    "kf = KFold(n_splits=3, random_state=1, shuffle=True)\n",
    "\n",
    "# Initialize an empty list to store cross-validation scores\n",
    "cv_scores = []\n",
    "\n",
    "# Perform KFold cross-validation\n",
    "for train_index, test_index in kf.split(titanic):\n",
    "    # Split the data into training and test sets\n",
    "    train_data, test_data = titanic.iloc[train_index], titanic.iloc[test_index]\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    rf_model.fit(train_data[predictors], train_data[\"Survived\"])\n",
    "    \n",
    "    # Calculate the accuracy on the test set\n",
    "    score = rf_model.score(test_data[predictors], test_data[\"Survived\"])\n",
    "    cv_scores.append(score)\n",
    "\n",
    "# Calculate the mean accuracy from the cross-validation\n",
    "mean_cv_score = np.mean(cv_scores)\n",
    "\n",
    "print(\"Cross-validated accuracy:\", mean_cv_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6e944d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Mlle          2\n",
      "Major         2\n",
      "Col           2\n",
      "Countess      1\n",
      "Capt          1\n",
      "Ms            1\n",
      "Sir           1\n",
      "Lady          1\n",
      "Mme           1\n",
      "Don           1\n",
      "Jonkheer      1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     182\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "8       4\n",
      "7       2\n",
      "15      1\n",
      "14      1\n",
      "11      1\n",
      "13      1\n",
      "12      1\n",
      "10      1\n",
      "9       1\n",
      "16      1\n",
      "Name: Name, dtype: int64\n",
      "                                                Name  Title\n",
      "0                            Braund, Mr. Owen Harris      1\n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...      3\n",
      "2                             Heikkinen, Miss. Laina      2\n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)      3\n",
      "4                           Allen, Mr. William Henry      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Generate a FamilySize column by adding the number of siblings/spouses (SibSp) and parents/children (Parch)\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# Use the .apply method to create a new series that calculates the length of each name\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))\n",
    "\n",
    "# Define a function to extract the title from a name\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title\n",
    "    title_search = re.search(r'([A-Za-z]+)\\.', name)\n",
    "    # If a title is found, extract and return it\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return None\n",
    "\n",
    "# Extract all titles from the names and print how often each one occurs\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "\n",
    "print(titles.value_counts())\n",
    "\n",
    "title_mapping = {\n",
    "    \"Mr\": 1,\n",
    "    \"Miss\": 2,\n",
    "    \"Mrs\": 3,\n",
    "    \"Master\": 4,\n",
    "    \"Dr\": 5,\n",
    "    \"Rev\": 6,\n",
    "    \"Mlle\": 7,\n",
    "    \"Major\": 8,\n",
    "    \"Col\": 8,       \n",
    "    \"Countess\": 9,\n",
    "    \"Capt\": 10,\n",
    "    \"Ms\": 11,\n",
    "    \"Sir\": 12,\n",
    "    \"Lady\": 13,\n",
    "    \"Mme\": 14,\n",
    "    \"Don\": 15,\n",
    "    \"Jonkheer\": 16\n",
    "}\n",
    "\n",
    "titles = titles.map(title_mapping)\n",
    "\n",
    "print(titles.value_counts())\n",
    "titanic[\"Title\"] = titles\n",
    "\n",
    "\n",
    "print(titanic[[\"Name\", \"Title\"]].head())\n",
    "# Print the counts of each title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "73e0104d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the predictors\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"FamilySize\", \"Title\", \"NameLength\",\"Embarked\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores to see which features are the best\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.ylabel('Score (-log10 p-value)')\n",
    "plt.title('Feature Selection Scores')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features based on the scores\n",
    "best_predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "# Initialize the Random Forest classifier with specified parameters\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=8, min_samples_leaf=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "09a1d393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8097643097643098\n",
      "Train Accuracy: 0.8838383838383839\n",
      "Train Accuracy: 0.7996632996632996\n",
      "Train Accuracy: 0.8636363636363636\n",
      "Train Accuracy: 0.7996632996632996\n",
      "Train Accuracy: 0.8501683501683501\n",
      "Accuracy: 0.8170594837261503\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume you have loaded and preprocessed the Titanic dataset into the variable titanic\n",
    "\n",
    "# Initialize algorithms\n",
    "algorithms = [\n",
    "    (LogisticRegression(random_state=1, max_iter=200), [\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"]),\n",
    "    (GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"])\n",
    "]\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize prediction lists\n",
    "full_test_predictions = []\n",
    "full_test_true_values = []\n",
    "\n",
    "# Train and predict for each fold\n",
    "for train_index, test_index in kf.split(titanic):\n",
    "    train_target = titanic[\"Survived\"].iloc[train_index]\n",
    "    \n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm\n",
    "        alg.fit(titanic[predictors].iloc[train_index, :], train_target)\n",
    "\n",
    "        # Predict on the training set and calculate accuracy\n",
    "        train_predictions = alg.predict(titanic[predictors].iloc[train_index, :])\n",
    "        train_accuracy = np.sum(train_predictions == train_target.values) / len(train_target)\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "\n",
    "        # Predict on the test set\n",
    "        test_predictions = alg.predict(titanic[predictors].iloc[test_index, :])\n",
    "        full_test_predictions.append(test_predictions)\n",
    "        full_test_true_values.append(titanic[\"Survived\"].iloc[test_index].values)\n",
    "\n",
    "# Combine all test predictions\n",
    "predictions = np.concatenate(full_test_predictions, axis=0)\n",
    "true_values = np.concatenate(full_test_true_values, axis=0)  # Combine true values\n",
    "\n",
    "# Check length match\n",
    "if len(predictions) == len(true_values):\n",
    "    accuracy = np.mean(predictions == true_values)  # Calculate accuracy\n",
    "else:\n",
    "    print(f\"Predictions length: {len(predictions)}, True values length: {len(true_values)}\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b21b52d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8097643097643098\n",
      "Train Accuracy: 0.8838383838383839\n",
      "Train Accuracy: 0.7996632996632996\n",
      "Train Accuracy: 0.8636363636363636\n",
      "Train Accuracy: 0.7996632996632996\n",
      "Train Accuracy: 0.8501683501683501\n",
      "Predictions : [0.09207493 0.58552235 0.91026849 0.24183647 0.55180036 0.04448276\n",
      " 0.66605564 0.21320086 0.19349697 0.80412724 0.38220749 0.41129113\n",
      " 0.28037826 0.53590884 0.82402917 0.09247282 0.6914474  0.67119854\n",
      " 0.1428069  0.82723019 0.11555942 0.82961564 0.11013326 0.13008156\n",
      " 0.89981754 0.28232615 0.83255954 0.29498673 0.07570333 0.0950967\n",
      " 0.0954652  0.10348414 0.09482621 0.85088192 0.48944253 0.10350319\n",
      " 0.09466748 0.26570776 0.05123407 0.10348414 0.06306627 0.61112144\n",
      " 0.05202477 0.08023195 0.70753735 0.73068212 0.14568755 0.49349985\n",
      " 0.38026599 0.16349009 0.05531714 0.37452175 0.48506048 0.19349697\n",
      " 0.23129815 0.47468536 0.62072863 0.1063068  0.12220669 0.07346725\n",
      " 0.05560168 0.80465133 0.08641557 0.16018379 0.20603275 0.36074437\n",
      " 0.30794999 0.08294598 0.90319183 0.88404996 0.04032467 0.08566769\n",
      " 0.71270917 0.11248744 0.58026736 0.92478664 0.07892651 0.31032764\n",
      " 0.22109276 0.37314068 0.2162896  0.16453531 0.77623876 0.17854214\n",
      " 0.11250638 0.6244631  0.59068945 0.7999097  0.26894915 0.62503698\n",
      " 0.91447343 0.94980563 0.19811461 0.1548026  0.08741143 0.8749642\n",
      " 0.6628641  0.15881725 0.09318175 0.7263949  0.94958941 0.10348414\n",
      " 0.35926021 0.09291813 0.95223727 0.92575098 0.91029935 0.13977895\n",
      " 0.89435333 0.10724424 0.93598166 0.28952874 0.29658636 0.5592783\n",
      " 0.81832545 0.07526784 0.10555136 0.11660045 0.09651187 0.0842766\n",
      " 0.15983983 0.64789498 0.41808635 0.09884477 0.11346757 0.08985325\n",
      " 0.9101406  0.23510346 0.14603294 0.61646335 0.10930344 0.122405\n",
      " 0.9260335  0.07288794 0.52593139 0.1515715  0.09464835 0.82184426\n",
      " 0.85064785 0.39312937 0.11765722 0.90909275 0.71577136 0.17174763\n",
      " 0.86473808 0.8444945  0.7046899  0.65031167 0.41995903 0.27979665\n",
      " 0.11878346 0.10933992 0.23872559 0.87005638 0.7132402  0.43100365\n",
      " 0.25615698 0.20941273 0.86382337 0.9122603  0.93547501 0.77528737\n",
      " 0.38248848 0.10624977 0.12978971 0.39282933 0.81713534 0.79361864\n",
      " 0.10732787 0.09360163 0.10348212 0.07892899 0.12723641 0.84862462\n",
      " 0.1378106  0.40722972 0.18011471 0.60610829 0.89051212 0.09550313\n",
      " 0.07069612 0.08695813 0.84824083 0.64789498 0.12002969 0.78292302\n",
      " 0.5296375  0.10575781 0.91085344 0.10257049 0.66120743 0.12342844\n",
      " 0.05942776 0.13008089 0.82819428 0.78304608 0.06389866 0.26755146\n",
      " 0.1169501  0.08437056 0.18920486 0.09221922 0.10931395 0.20951151\n",
      " 0.15455737 0.38357721 0.16595339 0.11135297 0.11741794 0.72512427\n",
      " 0.60093423 0.76902183 0.2010678  0.21628032 0.25645211 0.3323622\n",
      " 0.20946418 0.10257049 0.64755009 0.22820829 0.61163387 0.45716148\n",
      " 0.07173369 0.05964623 0.11556856 0.11435084 0.69101203 0.14022145\n",
      " 0.93096738 0.928383   0.09482438 0.08727426 0.3531174  0.75416831\n",
      " 0.54988745 0.927329   0.15213726 0.2010678  0.29915997 0.60629824\n",
      " 0.08294598 0.17881587 0.31642672 0.9056186  0.08098119 0.09141365\n",
      " 0.36488921 0.22068225 0.07635306 0.88008166 0.74164256 0.08989764\n",
      " 0.726195   0.19624582 0.11556204 0.35523374 0.22128871 0.2240581\n",
      " 0.10163039 0.14645805 0.83350079 0.62433521 0.16149873 0.68985586\n",
      " 0.46216466 0.07141409 0.3639156  0.30357849 0.15902747 0.12375817\n",
      " 0.09814317 0.78854859 0.15561312 0.62211366 0.07729003 0.7447267\n",
      " 0.06085941 0.22883179 0.07518053 0.47930334 0.13506666 0.70260263\n",
      " 0.08907519 0.8779716  0.7898495  0.81453069 0.0665432  0.86612026\n",
      " 0.8477128  0.74598719 0.11162387 0.11344613 0.16257185 0.60702385\n",
      " 0.40485233 0.92746643 0.44868385]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume you have loaded and preprocessed the Titanic dataset into the variable titanic\n",
    "\n",
    "# Initialize algorithms\n",
    "algorithms = [\n",
    "    (LogisticRegression(random_state=1, max_iter=200), [\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"]),\n",
    "    (GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"])\n",
    "]\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize prediction lists\n",
    "full_test_predictions = []\n",
    "full_test_true_values = []\n",
    "\n",
    "# Train and predict for each fold\n",
    "for train_index, test_index in kf.split(titanic):\n",
    "    train_target = titanic[\"Survived\"].iloc[train_index]\n",
    "    \n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm\n",
    "        alg.fit(titanic[predictors].iloc[train_index, :], train_target)\n",
    "\n",
    "        # Predict on the training set and calculate accuracy\n",
    "        train_predictions = alg.predict(titanic[predictors].iloc[train_index, :])\n",
    "        train_accuracy = np.sum(train_predictions == train_target.values) / len(train_target)\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "\n",
    "        # Predict on the test set\n",
    "        if hasattr(alg, \"predict_proba\"):  # Check if the algorithm has predict_proba method\n",
    "            test_predictions = alg.predict_proba(titanic[predictors].iloc[test_index, :])[:, 1]  # Probabilities for the positive class\n",
    "        else:\n",
    "            test_predictions = alg.predict(titanic[predictors].iloc[test_index, :])  # Binary predictions\n",
    "        \n",
    "        full_test_predictions.append(test_predictions)\n",
    "        full_test_true_values.append(titanic[\"Survived\"].iloc[test_index].values)\n",
    "\n",
    "# Combine all test predictions\n",
    "# Weighted average of predictions without rounding\n",
    "predictions = (full_test_predictions[0] * 3 + full_test_predictions[1]) / 4  # Weighted predictions\n",
    "true_values = np.concatenate(full_test_true_values, axis=0)  # Combine true values\n",
    "\n",
    "print(\"Predictions :\", predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "81979ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy for LogisticRegression: 0.8097643097643098\n",
      "Test Predictions (Probabilities) for LogisticRegression on fold: [0.07994108 0.6179674  0.91286357 0.2500144  0.57439258 0.01740917\n",
      " 0.6975629  0.21185094 0.21329588 0.81404837 0.41062434 0.44033305\n",
      " 0.27485203 0.53466049 0.82266654 0.09389307 0.70626054 0.69120832\n",
      " 0.12388583 0.83514506 0.10422543 0.80983764 0.07488206 0.12358828\n",
      " 0.90411065 0.17241229 0.84313251 0.35121798 0.05811227 0.09196087\n",
      " 0.08194319 0.09327877 0.08173487 0.85079483 0.47619632 0.09330417\n",
      " 0.08152323 0.25529136 0.03520577 0.09327877 0.04260292 0.65209953\n",
      " 0.03626003 0.06366583 0.70359673 0.73445643 0.16318657 0.54994468\n",
      " 0.46889863 0.1790687  0.04064986 0.26998616 0.48540607 0.21329588\n",
      " 0.27199417 0.52485869 0.65124446 0.11168899 0.11824217 0.02599405\n",
      " 0.04408217 0.79682942 0.07239527 0.16280147 0.23830698 0.38200684\n",
      " 0.29284111 0.06728453 0.92538089 0.88485561 0.01186505 0.07091349\n",
      " 0.72490474 0.11357989 0.61096075 0.93532418 0.06192524 0.31478454\n",
      " 0.25009027 0.44693086 0.24368605 0.17607031 0.78856191 0.1933561\n",
      " 0.10015471 0.62623676 0.62485687 0.79959939 0.25961321 0.62390447\n",
      " 0.92365183 0.96868283 0.19171859 0.16309336 0.07372307 0.86579119\n",
      " 0.66814949 0.16844622 0.07954226 0.75285721 0.95905826 0.09327877\n",
      " 0.38002797 0.09349637 0.96258875 0.93660996 0.92694594 0.09403195\n",
      " 0.89682503 0.09829225 0.9469637  0.29057006 0.29646283 0.45864893\n",
      " 0.82327208 0.05704701 0.09603507 0.10561347 0.08398242 0.06905869\n",
      " 0.18272531 0.64819065 0.44939334 0.08896752 0.10659001 0.07510425\n",
      " 0.91269306 0.27282572 0.09812498 0.65922208 0.10103784 0.11942677\n",
      " 0.93698666 0.05387382 0.50938451 0.15831544 0.08149772 0.81975332\n",
      " 0.84707916 0.40641363 0.11217622 0.9130206  0.67076784 0.18429676\n",
      " 0.98341577 0.83887469 0.66845767 0.72901585 0.45558641 0.29997148\n",
      " 0.12073912 0.10108649 0.23023667 0.8931283  0.72561279 0.46661641\n",
      " 0.26845193 0.20612626 0.85788683 0.91551932 0.94819695 0.75767747\n",
      " 0.39222577 0.09632261 0.12927305 0.35070524 0.82256691 0.78211916\n",
      " 0.09840375 0.07647178 0.09556183 0.06192854 0.12682304 0.83392208\n",
      " 0.15268398 0.47867357 0.20767217 0.64541533 0.88652176 0.08263743\n",
      " 0.06319799 0.08283785 0.83016004 0.64819065 0.11533951 0.77606883\n",
      " 0.48321544 0.09268002 0.91536818 0.09206058 0.66594059 0.13451784\n",
      " 0.04613068 0.1235874  0.91243501 0.76518655 0.0551448  0.2843217\n",
      " 0.11351914 0.06918397 0.20757307 0.08812423 0.10105185 0.20691446\n",
      " 0.1622966  0.47218471 0.12893121 0.10377054 0.1159117  0.69570351\n",
      " 0.63851658 0.77386607 0.22339032 0.21528304 0.2803646  0.34416395\n",
      " 0.2345855  0.09206058 0.67288882 0.22481688 0.59984251 0.50149353\n",
      " 0.06046286 0.04519238 0.10939133 0.10712405 0.68156298 0.14318204\n",
      " 0.94564376 0.98706724 0.08173243 0.07305557 0.37183755 0.72951872\n",
      " 0.57045421 0.9407926  0.16644632 0.22339032 0.32526516 0.64964994\n",
      " 0.06728453 0.16598694 0.32291664 0.92070493 0.07691142 0.07718479\n",
      " 0.42856129 0.24954293 0.05849397 0.87261448 0.76952281 0.07516344\n",
      " 0.70131312 0.20826768 0.10938265 0.36558987 0.2480919  0.26626334\n",
      " 0.0908071  0.15196729 0.93705799 0.65256231 0.17202153 0.62549858\n",
      " 0.34130287 0.06378939 0.43957787 0.33254391 0.16355696 0.11226422\n",
      " 0.08615748 0.97612464 0.14084982 0.53562103 0.07388253 0.75868752\n",
      " 0.0476938  0.26040897 0.0569306  0.47433324 0.10807213 0.6903804\n",
      " 0.07043652 0.89458978 0.78618579 0.8149224  0.05108544 0.85399927\n",
      " 0.85424471 0.74315323 0.10413174 0.10656142 0.1688268  0.64663607\n",
      " 0.45375828 0.93751883 0.42860166]\n",
      "Train Accuracy for GradientBoostingClassifier: 0.8838383838383839\n",
      "Test Predictions (Probabilities) for GradientBoostingClassifier on fold: [0.1284765  0.48818719 0.90248323 0.21730267 0.48402371 0.12570356\n",
      " 0.57153388 0.21725062 0.13410024 0.77436388 0.29695695 0.32416535\n",
      " 0.29695695 0.53965391 0.82811707 0.08821206 0.64700795 0.6111692\n",
      " 0.19957009 0.80348559 0.14956138 0.88894963 0.21588686 0.14956138\n",
      " 0.88693823 0.61206774 0.80084063 0.12629298 0.1284765  0.10450417\n",
      " 0.13603123 0.13410024 0.13410024 0.8511432  0.52918115 0.13410024\n",
      " 0.13410024 0.29695695 0.09931899 0.13410024 0.12445632 0.48818719\n",
      " 0.09931899 0.12993032 0.71935919 0.71935919 0.09319048 0.32416535\n",
      " 0.11436807 0.11675424 0.09931899 0.68812851 0.48402371 0.13410024\n",
      " 0.10921007 0.32416535 0.52918115 0.09016022 0.13410024 0.21588686\n",
      " 0.09016022 0.82811707 0.1284765  0.15233076 0.10921007 0.29695695\n",
      " 0.35327661 0.12993032 0.83662467 0.88163302 0.12570356 0.12993032\n",
      " 0.67612243 0.10921007 0.48818719 0.89317403 0.12993032 0.29695695\n",
      " 0.13410024 0.15177011 0.13410024 0.12993032 0.73926931 0.13410024\n",
      " 0.14956138 0.61914212 0.48818719 0.80084063 0.29695695 0.62843453\n",
      " 0.88693823 0.89317403 0.21730267 0.12993032 0.1284765  0.90248323\n",
      " 0.64700795 0.12993032 0.13410024 0.64700795 0.92118283 0.13410024\n",
      " 0.29695695 0.0911834  0.92118283 0.89317403 0.86035959 0.27701993\n",
      " 0.88693823 0.13410024 0.90303555 0.28640477 0.29695695 0.8611664\n",
      " 0.80348559 0.12993032 0.13410024 0.14956138 0.13410024 0.12993032\n",
      " 0.0911834  0.64700795 0.32416535 0.1284765  0.13410024 0.13410024\n",
      " 0.90248323 0.12193666 0.28975682 0.48818719 0.13410024 0.13133968\n",
      " 0.89317403 0.12993032 0.57557204 0.13133968 0.13410024 0.82811707\n",
      " 0.86135394 0.35327661 0.13410024 0.89730921 0.85078194 0.13410024\n",
      " 0.508705   0.86135394 0.81338657 0.41419912 0.31307688 0.21927214\n",
      " 0.11291648 0.13410024 0.26419235 0.80084063 0.67612243 0.32416535\n",
      " 0.21927214 0.21927214 0.88163302 0.90248323 0.89730921 0.82811707\n",
      " 0.35327661 0.13603123 0.13133968 0.51920163 0.80084063 0.82811707\n",
      " 0.13410024 0.1449912  0.12724299 0.12993032 0.1284765  0.89273223\n",
      " 0.09319048 0.19289818 0.09744236 0.48818719 0.90248323 0.13410024\n",
      " 0.09319048 0.09931899 0.90248323 0.64700795 0.13410024 0.80348559\n",
      " 0.66890367 0.1449912  0.89730921 0.13410024 0.64700795 0.09016022\n",
      " 0.09931899 0.14956138 0.57547211 0.83662467 0.09016022 0.21724073\n",
      " 0.12724299 0.12993032 0.13410024 0.10450417 0.13410024 0.21730267\n",
      " 0.13133968 0.1177547  0.27701993 0.13410024 0.12193666 0.81338657\n",
      " 0.48818719 0.75448909 0.13410024 0.21927214 0.18471465 0.29695695\n",
      " 0.13410024 0.13410024 0.57153388 0.23838253 0.64700795 0.32416535\n",
      " 0.1055462  0.1030078  0.13410024 0.13603123 0.71935919 0.13133968\n",
      " 0.88693823 0.75233027 0.13410024 0.12993032 0.29695695 0.82811707\n",
      " 0.48818719 0.88693823 0.10921007 0.13410024 0.2208444  0.47624313\n",
      " 0.12993032 0.21730267 0.29695695 0.86035959 0.09319048 0.13410024\n",
      " 0.17387297 0.13410024 0.12993032 0.90248323 0.65800182 0.13410024\n",
      " 0.80084063 0.16018027 0.13410024 0.32416535 0.14087914 0.09744236\n",
      " 0.13410024 0.12993032 0.52282918 0.53965391 0.12993032 0.88292771\n",
      " 0.82475003 0.09428822 0.1369288  0.21668222 0.14543899 0.15824003\n",
      " 0.13410024 0.22582042 0.199903   0.88159153 0.08751253 0.70284426\n",
      " 0.10035625 0.13410024 0.12993032 0.49421367 0.21605026 0.73926931\n",
      " 0.1449912  0.82811707 0.80084063 0.81335555 0.11291648 0.90248323\n",
      " 0.82811707 0.75448909 0.13410024 0.13410024 0.14380701 0.48818719\n",
      " 0.25813448 0.89730921 0.50893041]\n",
      "Train Accuracy for LogisticRegression: 0.7996632996632996\n",
      "Test Predictions (Probabilities) for LogisticRegression on fold: [0.91989175 0.08730097 0.16217148 0.86111598 0.82629467 0.11277502\n",
      " 0.57342201 0.46721182 0.0887551  0.91850237 0.95010395 0.11153236\n",
      " 0.12528223 0.52745217 0.6632422  0.18758341 0.12058739 0.90568982\n",
      " 0.29547391 0.33919897 0.11474575 0.19299315 0.35110115 0.10954213\n",
      " 0.75950088 0.42340383 0.91231017 0.11266806 0.05670628 0.06480953\n",
      " 0.48369147 0.75257374 0.16542386 0.43887756 0.0906411  0.0983943\n",
      " 0.34474591 0.10903698 0.11082492 0.08071257 0.21869571 0.2949904\n",
      " 0.10494119 0.54531441 0.77268833 0.1000396  0.47526784 0.31205114\n",
      " 0.05289757 0.75944679 0.09515433 0.0890944  0.10174885 0.10826077\n",
      " 0.15956303 0.46974111 0.16324542 0.59740497 0.11082492 0.88262343\n",
      " 0.08241541 0.59803637 0.61609652 0.23674783 0.56710098 0.06225194\n",
      " 0.0991744  0.094822   0.11662444 0.20115205 0.12182098 0.14621527\n",
      " 0.12058739 0.21044563 0.10972875 0.23428913 0.11139686 0.12718243\n",
      " 0.14088322 0.76685913 0.26731265 0.05698147 0.6953976  0.15909407\n",
      " 0.08118384 0.72323708 0.37414042 0.0887943  0.11468567 0.44631912\n",
      " 0.11801874 0.23164083 0.60232964 0.47943665 0.95992383 0.14217775\n",
      " 0.66041112 0.94605995 0.2211368  0.94309425 0.73999305 0.59190108\n",
      " 0.75733865 0.51877358 0.08305776 0.92716625 0.14914878 0.94080853\n",
      " 0.91078178 0.21681419 0.18349496 0.72724102 0.51922343 0.36911009\n",
      " 0.10461879 0.0879669  0.65014492 0.75260119 0.02914638 0.22797629\n",
      " 0.57579454 0.09477875 0.88549327 0.6417524  0.48552352 0.9458596\n",
      " 0.60929597 0.60673968 0.14303614 0.11453325 0.0903307  0.84627198\n",
      " 0.06392328 0.54659656 0.10890216 0.56882276 0.16639486 0.80283006\n",
      " 0.08139431 0.56042472 0.08269845 0.06586706 0.06709288 0.7168163\n",
      " 0.57325917 0.52461535 0.07176213 0.19193101 0.09193982 0.72653736\n",
      " 0.28653556 0.41909681 0.27202676 0.11356995 0.08367157 0.61314524\n",
      " 0.36939737 0.1256818  0.90934964 0.19193101 0.30998534 0.66559079\n",
      " 0.61265534 0.06357428 0.1842173  0.06714562 0.46948059 0.9093334\n",
      " 0.09515433 0.1104621  0.11089222 0.14305946 0.7417262  0.69568367\n",
      " 0.11089222 0.17144767 0.09186953 0.90503536 0.07769379 0.75031355\n",
      " 0.78634466 0.136043   0.9438067  0.11634861 0.89960562 0.85200906\n",
      " 0.35780093 0.41536592 0.80425668 0.3300886  0.14687404 0.14055804\n",
      " 0.25624596 0.9829887  0.54928173 0.14681984 0.4126714  0.88528238\n",
      " 0.09188283 0.91711205 0.14640943 0.28606185 0.08689922 0.07050963\n",
      " 0.13978014 0.39883709 0.07463014 0.43553588 0.40599802 0.78019882\n",
      " 0.64506552 0.75056722 0.11530237 0.64806214 0.76997005 0.09188283\n",
      " 0.3288861  0.07326652 0.97636955 0.68491204 0.10008879 0.7716826\n",
      " 0.11109882 0.64968257 0.10415643 0.08556517 0.09129903 0.21681419\n",
      " 0.09024603 0.91692265 0.70089384 0.10423709 0.19812325 0.89480044\n",
      " 0.11341881 0.03298434 0.17693844 0.36677057 0.80455869 0.36981583\n",
      " 0.07713528 0.97523971 0.36466046 0.08226228 0.18562329 0.14259065\n",
      " 0.36061056 0.9654759  0.10053151 0.81756442 0.09571545 0.14701207\n",
      " 0.11311022 0.75597098 0.10157385 0.5835155  0.08399859 0.76318658\n",
      " 0.77708027 0.33395003 0.09834585 0.97693011 0.87883647 0.14766629\n",
      " 0.10615733 0.19488815 0.94356948 0.71285775 0.92175791 0.39623498\n",
      " 0.62750329 0.14879914 0.09340741 0.17512757 0.89490706 0.10180503\n",
      " 0.0943414  0.60565512 0.07533875 0.17156535 0.33845399 0.10903698\n",
      " 0.41426742 0.91752068 0.11894175 0.04361193 0.89947882 0.62128508\n",
      " 0.90808046 0.2979548  0.56485086 0.22005032 0.10180917 0.07113869\n",
      " 0.20507806 0.10312762 0.15408737]\n",
      "Train Accuracy for GradientBoostingClassifier: 0.8636363636363636\n",
      "Test Predictions (Probabilities) for GradientBoostingClassifier on fold: [0.84933041 0.14178635 0.15957664 0.85180145 0.85596683 0.11799673\n",
      " 0.53034583 0.20179862 0.14178635 0.39990385 0.88404827 0.35142649\n",
      " 0.13394974 0.53034583 0.54889202 0.182816   0.09360689 0.88404827\n",
      " 0.21623364 0.37238105 0.1013536  0.15284792 0.71326987 0.12747801\n",
      " 0.63347298 0.30907974 0.84859426 0.11799673 0.21347344 0.15304478\n",
      " 0.2682581  0.83847484 0.18406823 0.3073458  0.15284792 0.14178635\n",
      " 0.30907974 0.11799673 0.11799673 0.35177292 0.26161511 0.27174784\n",
      " 0.11214319 0.61187433 0.83847484 0.18209596 0.16544637 0.32701725\n",
      " 0.15284792 0.63347298 0.14178635 0.15284792 0.18209596 0.18340323\n",
      " 0.54311674 0.15780645 0.19915542 0.65518302 0.11799673 0.86565739\n",
      " 0.12813685 0.71113515 0.53862515 0.1013536  0.53098576 0.15284792\n",
      " 0.15284792 0.14469082 0.09360689 0.15284792 0.14811326 0.28257974\n",
      " 0.09360689 0.19558561 0.12747801 0.1013536  0.11214319 0.17257026\n",
      " 0.18406823 0.89387612 0.36466719 0.15669835 0.83847484 0.14811326\n",
      " 0.18340323 0.83847484 0.30907974 0.35177292 0.09360689 0.15780645\n",
      " 0.15957664 0.15957664 0.46568651 0.37610713 0.88404827 0.15982949\n",
      " 0.69598084 0.88404827 0.17488716 0.88404827 0.83847484 0.53098576\n",
      " 0.83847484 0.53034583 0.11182119 0.87873618 0.15982949 0.88404827\n",
      " 0.84859426 0.19558561 0.15284792 0.80026012 0.53034583 0.30907974\n",
      " 0.10286849 0.21347344 0.5392636  0.63347298 0.16393633 0.26161511\n",
      " 0.45175762 0.12813685 0.88092839 0.40581785 0.27621944 0.88404827\n",
      " 0.53098576 0.47866284 0.12932719 0.09360689 0.32557729 0.81604792\n",
      " 0.14178635 0.53034583 0.11799673 0.53098576 0.18406823 0.80026012\n",
      " 0.14178635 0.46568651 0.16764163 0.15304478 0.15304478 0.80026012\n",
      " 0.567753   0.46568651 0.13848634 0.10829697 0.14178635 0.83847484\n",
      " 0.51706424 0.16544637 0.19971787 0.12747801 0.18209596 0.68889517\n",
      " 0.34355766 0.14811326 0.85963653 0.10829697 0.34355766 0.83847484\n",
      " 0.46568651 0.106201   0.12813685 0.15304478 0.6052785  0.87873618\n",
      " 0.14178635 0.11214319 0.11799673 0.14662375 0.63347298 0.63347298\n",
      " 0.11799673 0.17488716 0.14178635 0.89464426 0.14469082 0.30269226\n",
      " 0.28005296 0.12932719 0.88404827 0.12747801 0.86565739 0.84271078\n",
      " 0.16544637 0.44972113 0.83847484 0.53078754 0.14811326 0.12932719\n",
      " 0.24764032 0.86565739 0.53034583 0.14811326 0.24837386 0.88975707\n",
      " 0.14178635 0.88404827 0.18406823 0.27174784 0.12813685 0.13848634\n",
      " 0.09360689 0.30907974 0.15304478 0.34355766 0.37610713 0.85180145\n",
      " 0.63287621 0.91881054 0.14022267 0.39990385 0.80026012 0.14178635\n",
      " 0.16544637 0.16764163 0.84933041 0.36191258 0.18209596 0.83847484\n",
      " 0.12747801 0.63287621 0.13394974 0.12813685 0.11799673 0.19558561\n",
      " 0.14178635 0.85963653 0.83847484 0.35142649 0.15284792 0.47488532\n",
      " 0.12747801 0.12788738 0.32316371 0.51706424 0.75939686 0.44972113\n",
      " 0.12813685 0.88404827 0.52471713 0.18209596 0.18406823 0.39522617\n",
      " 0.34355766 0.88404827 0.17998275 0.91881054 0.0888414  0.15284792\n",
      " 0.12747801 0.63347298 0.16764163 0.36635536 0.11182119 0.80026012\n",
      " 0.91881054 0.67803924 0.14178635 0.87873618 0.87873618 0.22217111\n",
      " 0.12747801 0.10829697 0.87873618 0.65872514 0.88975707 0.30907974\n",
      " 0.53098576 0.72330368 0.14178635 0.15284792 0.88274517 0.18209596\n",
      " 0.14178635 0.53098576 0.12813685 0.18113881 0.72330368 0.11799673\n",
      " 0.30907974 0.86565739 0.1013536  0.35177292 0.85963653 0.46568651\n",
      " 0.87873618 0.3177165  0.61187433 0.12747801 0.18209596 0.15284792\n",
      " 0.15284792 0.1736888  0.14811326]\n",
      "Train Accuracy for LogisticRegression: 0.7996632996632996\n",
      "Test Predictions (Probabilities) for LogisticRegression on fold: [0.17607084 0.69359821 0.69088744 0.21656294 0.21424848 0.77360571\n",
      " 0.55052738 0.26679106 0.11776923 0.59148989 0.67941131 0.14113286\n",
      " 0.84961385 0.75047156 0.104546   0.14102688 0.80446197 0.37537573\n",
      " 0.13775391 0.55283083 0.32856794 0.33744851 0.12625968 0.04405681\n",
      " 0.61733093 0.07178126 0.10988108 0.20541908 0.62849851 0.81441422\n",
      " 0.63078984 0.78631947 0.10702749 0.07093223 0.11474509 0.14459626\n",
      " 0.8982851  0.35354572 0.16206842 0.45703708 0.67171822 0.18884499\n",
      " 0.31083272 0.22407954 0.37197824 0.93590772 0.05203436 0.29425592\n",
      " 0.157541   0.79726821 0.27003213 0.13482952 0.23439566 0.29113658\n",
      " 0.10910435 0.23642622 0.09691617 0.42671694 0.06934938 0.82124093\n",
      " 0.4617953  0.20445387 0.67941131 0.82821565 0.0847556  0.76781687\n",
      " 0.40605845 0.12957083 0.77754394 0.1369929  0.9239387  0.23485103\n",
      " 0.05955841 0.47510524 0.9110928  0.112932   0.27082848 0.65739383\n",
      " 0.70830745 0.13694121 0.55147057 0.22447942 0.08988994 0.46489046\n",
      " 0.90301843 0.34673754 0.58367869 0.05774476 0.92788819 0.30767557\n",
      " 0.1233368  0.77062319 0.49624149 0.16304117 0.51253673 0.11618376\n",
      " 0.16220305 0.13726021 0.1745431  0.93352209 0.14886139 0.948125\n",
      " 0.44956913 0.14547914 0.75567668 0.11619871 0.8282581  0.24831147\n",
      " 0.9037961  0.1195027  0.8414993  0.00840868 0.93508702 0.04379054\n",
      " 0.80181024 0.48039276 0.42942959 0.42506815 0.07118046 0.24809314\n",
      " 0.25148345 0.91942856 0.75548376 0.62820549 0.99497851 0.94258695\n",
      " 0.74147946 0.06705701 0.76703458 0.30152596 0.1410251  0.93999052\n",
      " 0.46639575 0.12294573 0.17906396 0.45549817 0.45140156 0.25148268\n",
      " 0.73634555 0.23642622 0.12479184 0.10244161 0.60962518 0.65826836\n",
      " 0.1299906  0.11254324 0.1080557  0.1530905  0.20831728 0.67654504\n",
      " 0.09804952 0.32712734 0.08708836 0.24554732 0.25488425 0.69481678\n",
      " 0.12264428 0.08753788 0.49041938 0.17906396 0.08989884 0.13710494\n",
      " 0.3895342  0.21730996 0.1080557  0.2682629  0.23552369 0.12986536\n",
      " 0.157541   0.56110674 0.58425475 0.12155848 0.85614599 0.12974681\n",
      " 0.32368766 0.92865524 0.67654504 0.11513393 0.39330542 0.61417537\n",
      " 0.67756213 0.41114547 0.21328449 0.30280434 0.06623401 0.82522216\n",
      " 0.08238895 0.24841957 0.67891689 0.14894664 0.15500342 0.10375266\n",
      " 0.39796056 0.15189905 0.74995142 0.44491657 0.15372461 0.13732411\n",
      " 0.87137909 0.06130101 0.73868179 0.10982833 0.9211555  0.11919172\n",
      " 0.73133192 0.06301815 0.63942459 0.11098611 0.31771076 0.32885004\n",
      " 0.94256163 0.12288758 0.14187916 0.05936359 0.46881841 0.54145194\n",
      " 0.32612658 0.14894664 0.1262211  0.67947165 0.21884645 0.10376619\n",
      " 0.45957764 0.17848252 0.07549927 0.15296581 0.1499583  0.96098434\n",
      " 0.76876929 0.14291978 0.56607406 0.13355992 0.07331058 0.75275356\n",
      " 0.353197   0.2774899  0.13335764 0.14883768 0.81289623 0.21424848\n",
      " 0.22808307 0.25148268 0.27890594 0.16144187 0.13759313 0.27951643\n",
      " 0.10135746 0.13353972 0.6093575  0.09804952 0.11531032 0.16351278\n",
      " 0.16138467 0.67594017 0.06508698 0.46401375 0.78122184 0.10109842\n",
      " 0.08002361 0.12602831 0.1261054  0.15875046 0.28654043 0.468271\n",
      " 0.12453737 0.55544946 0.75036203 0.11916418 0.35397128 0.69913552\n",
      " 0.20750473 0.82787376 0.62911073 0.15446499 0.8465699  0.13352289\n",
      " 0.15320392 0.86833366 0.14139278 0.12067581 0.14497527 0.10949224\n",
      " 0.011886   0.10844462 0.92790806 0.71863061 0.05104082 0.24181378\n",
      " 0.1415883  0.45742339 0.24247206 0.40288922 0.85853671 0.82368985\n",
      " 0.10084745 0.48825223 0.51528899]\n",
      "Train Accuracy for GradientBoostingClassifier: 0.8501683501683501\n",
      "Test Predictions (Probabilities) for GradientBoostingClassifier on fold: [0.46643773 0.63424879 0.84457466 0.12595405 0.12595405 0.60119569\n",
      " 0.43419611 0.16015383 0.17430539 0.62658751 0.6635798  0.14364395\n",
      " 0.89439309 0.62658751 0.14364395 0.14364395 0.87285815 0.34323603\n",
      " 0.17846471 0.32608182 0.16015383 0.28880609 0.17430539 0.12595405\n",
      " 0.58432074 0.17990243 0.17430539 0.22413467 0.62658751 0.30114639\n",
      " 0.58432074 0.87285815 0.12595405 0.09966489 0.12595405 0.12408734\n",
      " 0.88075009 0.53393398 0.14364395 0.52484297 0.6635798  0.12408734\n",
      " 0.14364395 0.17139418 0.58404511 0.89361948 0.12595405 0.27509843\n",
      " 0.14364395 0.84349119 0.13031616 0.51269414 0.22413467 0.26275203\n",
      " 0.16015383 0.17430539 0.16015383 0.83125849 0.12595405 0.87285815\n",
      " 0.83125849 0.14364395 0.6635798  0.87285815 0.17430539 0.67900474\n",
      " 0.34323603 0.12408734 0.87285815 0.14364395 0.89361948 0.17430539\n",
      " 0.12595405 0.16015383 0.89361948 0.17430539 0.14364395 0.69148838\n",
      " 0.60119569 0.12408734 0.58432074 0.43834084 0.17430539 0.58432074\n",
      " 0.86926271 0.14105111 0.6635798  0.14174326 0.89361948 0.26275203\n",
      " 0.13909002 0.84349119 0.60783144 0.16015383 0.58432074 0.17430539\n",
      " 0.14364395 0.14364395 0.12595405 0.89361948 0.14364395 0.90819413\n",
      " 0.60783144 0.12408734 0.87285815 0.17430539 0.87285815 0.590513\n",
      " 0.89361948 0.17430539 0.87285815 0.22807074 0.89361948 0.09966489\n",
      " 0.87285815 0.25888024 0.30609203 0.39335738 0.12595405 0.17430539\n",
      " 0.63424879 0.89361948 0.87285815 0.69148838 0.79306906 0.89361948\n",
      " 0.63424879 0.12595405 0.87285815 0.27474826 0.14364395 0.89361948\n",
      " 0.6456308  0.17430539 0.12595405 0.83125849 0.16015383 0.13909002\n",
      " 0.87285815 0.17430539 0.17430539 0.14364395 0.58432074 0.87285815\n",
      " 0.14364395 0.19597102 0.14364395 0.14364395 0.43834084 0.84457466\n",
      " 0.12595405 0.21155281 0.09966489 0.13909002 0.14364395 0.63424879\n",
      " 0.17430539 0.12595405 0.31434033 0.12595405 0.13909002 0.14364395\n",
      " 0.16015383 0.60119569 0.14364395 0.26275203 0.14945004 0.14364395\n",
      " 0.14364395 0.58432074 0.34003152 0.17430539 0.86926271 0.14364395\n",
      " 0.26275203 0.89361948 0.84457466 0.14364395 0.52484297 0.58432074\n",
      " 0.67000866 0.16015383 0.17240844 0.14364395 0.12595405 0.27096114\n",
      " 0.12595405 0.17430539 0.58432074 0.14364395 0.14364395 0.17430539\n",
      " 0.5270982  0.14364395 0.87285815 0.28236033 0.12595405 0.14364395\n",
      " 0.86926271 0.12595405 0.87285815 0.17430539 0.89361948 0.13909002\n",
      " 0.87285815 0.12595405 0.58432074 0.17430539 0.43859595 0.52637534\n",
      " 0.89361948 0.17430539 0.2678999  0.09966489 0.16582633 0.17846471\n",
      " 0.27509843 0.14364395 0.17430539 0.62658751 0.28880609 0.17430539\n",
      " 0.61642989 0.3382154  0.12595405 0.14364395 0.14364395 0.90008516\n",
      " 0.63424879 0.17430539 0.61642989 0.12595405 0.12595405 0.87285815\n",
      " 0.43184978 0.21155281 0.12595405 0.14364395 0.87285815 0.12595405\n",
      " 0.17430539 0.13909002 0.14364395 0.14364395 0.17430539 0.62341307\n",
      " 0.12595405 0.14364395 0.87285815 0.12595405 0.17430539 0.14364395\n",
      " 0.14364395 0.6635798  0.12595405 0.8206917  0.68794885 0.12595405\n",
      " 0.13272462 0.13909002 0.17430539 0.16015383 0.17846471 0.37847458\n",
      " 0.17430539 0.25888024 0.87285815 0.13909002 0.14105111 0.62658751\n",
      " 0.12595405 0.86926271 0.58432074 0.45533315 0.84349119 0.14364395\n",
      " 0.14364395 0.89361948 0.14364395 0.51269414 0.14364395 0.09966489\n",
      " 0.28754454 0.12595405 0.89361948 0.87285815 0.12595405 0.14364395\n",
      " 0.16015383 0.31434033 0.63424879 0.14060165 0.86926271 0.87285815\n",
      " 0.12595405 0.58404511 0.37847458]\n",
      "Combined Predictions: [0 1 1 ... 0 1 0]\n",
      "Combined True Values: [0 1 1 ... 0 0 1]\n",
      "Unique Predictions: [0 1]\n",
      "Final Accuracy: 0.8170594837261503\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume you have loaded and preprocessed the Titanic dataset into the variable titanic\n",
    "\n",
    "# Initialize algorithms\n",
    "algorithms = [\n",
    "    (LogisticRegression(random_state=1, max_iter=200), [\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"]),\n",
    "    (GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"])\n",
    "]\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize prediction lists\n",
    "full_test_predictions = []\n",
    "full_test_true_values = []\n",
    "\n",
    "# Train and predict for each fold\n",
    "for train_index, test_index in kf.split(titanic):\n",
    "    train_target = titanic[\"Survived\"].iloc[train_index]\n",
    "\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm\n",
    "        alg.fit(titanic[predictors].iloc[train_index, :], train_target)\n",
    "\n",
    "        # Predict on the training set and calculate accuracy\n",
    "        train_predictions = alg.predict(titanic[predictors].iloc[train_index, :])\n",
    "        train_accuracy = np.sum(train_predictions == train_target.values) / len(train_target)\n",
    "        print(f\"Train Accuracy for {alg.__class__.__name__}: {train_accuracy}\")\n",
    "\n",
    "        # Predict on the test set\n",
    "        if hasattr(alg, \"predict_proba\"):  # Check if the algorithm has predict_proba method\n",
    "            test_predictions = alg.predict_proba(titanic[predictors].iloc[test_index, :])[:, 1]  # Probabilities for the positive class\n",
    "            print(f\"Test Predictions (Probabilities) for {alg.__class__.__name__} on fold: {test_predictions}\")  # Debugging\n",
    "            test_predictions = (test_predictions >= 0.5).astype(int)  # Convert probabilities to binary\n",
    "        else:\n",
    "            test_predictions = alg.predict(titanic[predictors].iloc[test_index, :])  # Binary predictions\n",
    "        \n",
    "        full_test_predictions.append(test_predictions)\n",
    "        full_test_true_values.append(titanic[\"Survived\"].iloc[test_index].values)\n",
    "\n",
    "# Combine all test predictions and true values\n",
    "all_test_predictions = np.concatenate(full_test_predictions, axis=0)\n",
    "all_test_true_values = np.concatenate(full_test_true_values, axis=0)\n",
    "\n",
    "# Debugging: Check the final predictions\n",
    "print(f\"Combined Predictions: {all_test_predictions}\")\n",
    "print(f\"Combined True Values: {all_test_true_values}\")\n",
    "\n",
    "# Check unique values in predictions\n",
    "print(f\"Unique Predictions: {np.unique(all_test_predictions)}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "if all_test_predictions.shape[0] == all_test_true_values.shape[0]:\n",
    "    accuracy = np.mean(all_test_predictions == all_test_true_values)  # Calculate accuracy\n",
    "else:\n",
    "    print(f\"Shape mismatch! Predictions length: {all_test_predictions.shape[0]}, True values length: {all_test_true_values.shape[0]}\")\n",
    "    accuracy = 0.0\n",
    "\n",
    "print(f\"Final Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b6f3c3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Predictions (Binary): [0 1 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0\n",
      " 0 0 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0\n",
      " 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 1 0 1\n",
      " 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1\n",
      " 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0\n",
      " 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0\n",
      " 0 0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 1 0 1\n",
      " 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0\n",
      " 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0\n",
      " 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0\n",
      " 1 0 1 0 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1\n",
      " 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0\n",
      " 0 0 0 1 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 0 0 1 0 1 0\n",
      " 0 0 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1\n",
      " 0 1 0]\n",
      "Combined True Values: [0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1\n",
      " 1 1 0 1 0 1 0 0 1 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
      " 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 1 1 1 1\n",
      " 0 1 0 0 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0\n",
      " 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 1\n",
      " 0 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 0 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 0 1\n",
      " 0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0\n",
      " 0 0 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 1\n",
      " 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 1 0\n",
      " 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0\n",
      " 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0\n",
      " 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 0 0 0 1 1 0\n",
      " 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1\n",
      " 1 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0\n",
      " 0 0 0 1 1 0 1 1 0 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0\n",
      " 0 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 1 1 0\n",
      " 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1\n",
      " 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 1\n",
      " 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1\n",
      " 0 0 1]\n",
      "Final Accuracy: 0.8215488215488216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, StackingClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume you have loaded and preprocessed the Titanic dataset into the variable titanic\n",
    "\n",
    "# Initialize algorithms\n",
    "algorithms = [\n",
    "    ('logistic', LogisticRegression(random_state=1, max_iter=200)),\n",
    "    ('gbc', GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3))\n",
    "]\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize prediction lists\n",
    "full_test_predictions = []\n",
    "full_test_true_values = []\n",
    "\n",
    "# Train and predict for each fold\n",
    "for train_index, test_index in kf.split(titanic):\n",
    "    train_target = titanic[\"Survived\"].iloc[train_index]\n",
    "\n",
    "    # Create an empty list to store predictions from each model\n",
    "    fold_predictions = []\n",
    "\n",
    "    for name, alg in algorithms:\n",
    "        # Fit the algorithm\n",
    "        alg.fit(titanic.iloc[train_index][[\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"]], train_target)\n",
    "\n",
    "        # Predict on the test set\n",
    "        if hasattr(alg, \"predict_proba\"):\n",
    "            test_predictions = alg.predict_proba(titanic.iloc[test_index][[\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"]])[:, 1]\n",
    "            fold_predictions.append(test_predictions)  # Store the probabilities for stacking\n",
    "        else:\n",
    "            test_predictions = alg.predict(titanic.iloc[test_index][[\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"]])\n",
    "            fold_predictions.append(test_predictions)\n",
    "\n",
    "    # Stack the predictions from both models\n",
    "    fold_predictions = np.column_stack(fold_predictions)  # Stack horizontally\n",
    "\n",
    "    # Use a simple Logistic Regression as the meta-model for stacking\n",
    "    meta_model = LogisticRegression()\n",
    "    meta_model.fit(fold_predictions, titanic[\"Survived\"].iloc[test_index])  # Train on fold predictions\n",
    "    meta_predictions = meta_model.predict_proba(fold_predictions)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "    full_test_predictions.append(meta_predictions)\n",
    "    full_test_true_values.append(titanic[\"Survived\"].iloc[test_index].values)\n",
    "\n",
    "# Combine all test predictions and true values\n",
    "all_test_predictions = np.concatenate(full_test_predictions, axis=0)\n",
    "all_test_true_values = np.concatenate(full_test_true_values, axis=0)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "all_test_predictions_binary = (all_test_predictions >= 0.5).astype(int)\n",
    "\n",
    "# Debugging: Check the final predictions\n",
    "print(f\"Combined Predictions (Binary): {all_test_predictions_binary}\")\n",
    "print(f\"Combined True Values: {all_test_true_values}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "if all_test_predictions_binary.shape[0] == all_test_true_values.shape[0]:\n",
    "    accuracy = np.mean(all_test_predictions_binary == all_test_true_values)  # Calculate accuracy\n",
    "else:\n",
    "    print(f\"Shape mismatch! Predictions length: {all_test_predictions_binary.shape[0]}, True values length: {all_test_true_values.shape[0]}\")\n",
    "    accuracy = 0.0\n",
    "\n",
    "print(f\"Final Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0468dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Predictions (Binary): [1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1\n",
      " 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0\n",
      " 1 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0\n",
      " 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
      " 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0\n",
      " 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0\n",
      " 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1\n",
      " 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0\n",
      " 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0\n",
      " 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0\n",
      " 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1\n",
      " 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0\n",
      " 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1\n",
      " 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
      " 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 1 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0\n",
      " 1 1 0]\n",
      "Combined True Values: [1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1\n",
      " 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0\n",
      " 1 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0\n",
      " 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
      " 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0\n",
      " 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0\n",
      " 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1\n",
      " 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0\n",
      " 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0\n",
      " 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0\n",
      " 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0\n",
      " 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1\n",
      " 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0\n",
      " 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1\n",
      " 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
      " 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 1 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
      " 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0\n",
      " 1 1 0]\n",
      "Final Accuracy: 0.9978\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume you have loaded and preprocessed the Titanic dataset into the variable titanic\n",
    "\n",
    "# Initialize algorithms\n",
    "algorithms = [\n",
    "    ('logistic', LogisticRegression(random_state=1, max_iter=200)),\n",
    "    ('gbc', GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)),\n",
    "    ('rf', RandomForestClassifier(random_state=1, n_estimators=100))  # Adding Random Forest\n",
    "]\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)  # Increase folds to 5 for better estimation\n",
    "\n",
    "# Initialize prediction lists\n",
    "full_test_predictions = []\n",
    "full_test_true_values = []\n",
    "\n",
    "# Train and predict for each fold\n",
    "for train_index, test_index in kf.split(titanic):\n",
    "    train_target = titanic[\"Survived\"].iloc[train_index]\n",
    "\n",
    "    # Create an empty list to store predictions from each model\n",
    "    fold_predictions = []\n",
    "\n",
    "    for name, alg in algorithms:\n",
    "        # Fit the algorithm\n",
    "        alg.fit(titanic.iloc[train_index][[\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"]], train_target)\n",
    "\n",
    "        # Predict on the test set\n",
    "        if hasattr(alg, \"predict_proba\"):\n",
    "            test_predictions = alg.predict_proba(titanic.iloc[test_index][[\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"]])[:, 1]\n",
    "            fold_predictions.append(test_predictions)  # Store the probabilities for stacking\n",
    "        else:\n",
    "            test_predictions = alg.predict(titanic.iloc[test_index][[\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"]])\n",
    "            fold_predictions.append(test_predictions)\n",
    "\n",
    "    # Stack the predictions from all models\n",
    "    fold_predictions = np.column_stack(fold_predictions)  # Stack horizontally\n",
    "\n",
    "    # Use a more complex model as the meta-model for stacking\n",
    "    meta_model = RandomForestClassifier(random_state=1, n_estimators=100)  # Random Forest as meta-model\n",
    "    meta_model.fit(fold_predictions, titanic[\"Survived\"].iloc[test_index])  # Train on fold predictions\n",
    "    meta_predictions = meta_model.predict_proba(fold_predictions)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "    full_test_predictions.append(meta_predictions)\n",
    "    full_test_true_values.append(titanic[\"Survived\"].iloc[test_index].values)\n",
    "\n",
    "# Combine all test predictions and true values\n",
    "all_test_predictions = np.concatenate(full_test_predictions, axis=0)\n",
    "all_test_true_values = np.concatenate(full_test_true_values, axis=0)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "all_test_predictions_binary = (all_test_predictions >= 0.5).astype(int)\n",
    "\n",
    "# Debugging: Check the final predictions\n",
    "print(f\"Combined Predictions (Binary): {all_test_predictions_binary}\")\n",
    "print(f\"Combined True Values: {all_test_true_values}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_test_true_values, all_test_predictions_binary)\n",
    "print(f\"Final Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d7c01349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Predictions (Binary): [1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1\n",
      " 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0\n",
      " 1 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0\n",
      " 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
      " 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0\n",
      " 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0\n",
      " 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1\n",
      " 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0\n",
      " 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0\n",
      " 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0\n",
      " 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1\n",
      " 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0\n",
      " 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1\n",
      " 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
      " 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0\n",
      " 1 1 0]\n",
      "Combined True Values: [1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1\n",
      " 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 0 0\n",
      " 1 1 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0\n",
      " 0 0 0 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1\n",
      " 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 0\n",
      " 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 0 1 1 0\n",
      " 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 1\n",
      " 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0\n",
      " 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0\n",
      " 1 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0\n",
      " 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 1 1 0\n",
      " 0 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1\n",
      " 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 0 0 1 0\n",
      " 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1\n",
      " 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
      " 1 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0\n",
      " 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0\n",
      " 1 0 0 0 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0\n",
      " 0 1 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1\n",
      " 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0\n",
      " 1 1 0]\n",
      "Final Accuracy: 0.9944\n",
      "Submission file 'submission2.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume you have loaded and preprocessed the Titanic dataset into the variable titanic\n",
    "\n",
    "# Initialize algorithms\n",
    "algorithms = [\n",
    "    ('logistic', LogisticRegression(random_state=1, max_iter=200)),\n",
    "    ('gbc', GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3)),\n",
    "    ('rf', RandomForestClassifier(random_state=1, n_estimators=100))  # Adding Random Forest\n",
    "]\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)  # Increase folds to 5 for better estimation\n",
    "\n",
    "# Initialize prediction lists\n",
    "full_test_predictions = []\n",
    "full_test_true_values = []\n",
    "\n",
    "# Train and predict for each fold\n",
    "for train_index, test_index in kf.split(titanic):\n",
    "    train_target = titanic[\"Survived\"].iloc[train_index]\n",
    "\n",
    "    # Create an empty list to store predictions from each model\n",
    "    fold_predictions = []\n",
    "\n",
    "    for name, alg in algorithms:\n",
    "        # Fit the algorithm\n",
    "        alg.fit(titanic.iloc[train_index][[\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"]], train_target)\n",
    "\n",
    "        # Predict on the test set\n",
    "        if hasattr(alg, \"predict_proba\"):\n",
    "            test_predictions = alg.predict_proba(titanic.iloc[test_index][[\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"]])[:, 1]\n",
    "            fold_predictions.append(test_predictions)  # Store the probabilities for stacking\n",
    "        else:\n",
    "            test_predictions = alg.predict(titanic.iloc[test_index][[\"Pclass\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Sex\", \"Embarked\"]])\n",
    "            fold_predictions.append(test_predictions)\n",
    "\n",
    "    # Stack the predictions from all models\n",
    "    fold_predictions = np.column_stack(fold_predictions)  # Stack horizontally\n",
    "\n",
    "    # Use a more complex model as the meta-model for stacking\n",
    "    meta_model = RandomForestClassifier(random_state=1, n_estimators=100)  # Random Forest as meta-model\n",
    "    meta_model.fit(fold_predictions, titanic[\"Survived\"].iloc[test_index])  # Train on fold predictions\n",
    "    meta_predictions = meta_model.predict_proba(fold_predictions)[:, 1]  # Get probabilities for the positive class\n",
    "\n",
    "    full_test_predictions.append(meta_predictions)\n",
    "    full_test_true_values.append(titanic[\"Survived\"].iloc[test_index].values)\n",
    "\n",
    "# Combine all test predictions and true values\n",
    "all_test_predictions = np.concatenate(full_test_predictions, axis=0)\n",
    "all_test_true_values = np.concatenate(full_test_true_values, axis=0)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "all_test_predictions_binary = (all_test_predictions >= 0.5).astype(int)\n",
    "\n",
    "# Debugging: Check the final predictions\n",
    "print(f\"Combined Predictions (Binary): {all_test_predictions_binary}\")\n",
    "print(f\"Combined True Values: {all_test_true_values}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(all_test_true_values, all_test_predictions_binary)\n",
    "print(f\"Final Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Assuming 'all_test_predictions_binary' is the correct length of 418 for test set predictions\n",
    "submission = pd.read_csv('gender_submission.csv')  # Load the sample submission file\n",
    "submission['Survived'] = all_test_predictions_binary[:418]  # Update with your predictions\n",
    "submission.to_csv('submission2.csv', index=False)  # Save to a new CSV file\n",
    "print(\"Submission file 'submission2.csv' created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584dd4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772290c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
